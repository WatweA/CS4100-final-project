{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa202fdd",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_market_calendars as mcal\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "from sklearn.metrics import mean_squared_error, median_absolute_error\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "# from tensorflow.keras import Sequential, Model\n",
    "# from tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, Dropout, Input\n",
    "# from tensorflow.keras.layers import concatenate\n",
    "# from tensorflow.keras.metrics import MeanSquaredError, RootMeanSquaredError\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "# from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c5f70",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### define the tickers and indicators used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97830864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exchange tickers\n",
    "tickers = list([\n",
    "    \"SPY\",  # S&P 500 Index Fund\n",
    "    \"IWV\",  # Russell 3000 Index Fund\n",
    "    \"QQQ\",  # Technology Sector Fund\n",
    "    \"IYF\",  # Financials Sector Fund\n",
    "    \"XLP\",  # Consumer Staples Sector Fund\n",
    "    \"XLU\",  # Utilities Sector Funds\n",
    "    \"XLV\",  # Health Care Sector Funds\n",
    "    \"IGE\",  # NA Natural Resources ETF\n",
    "    \"XLE\"  # Energy Sector Fund\n",
    "])\n",
    "\n",
    "# alias and FRED indicator\n",
    "indicators = dict({\n",
    "    \"3M_TBILL\": \"DTB3\",  # 3-Month Treasury Bill: Secondary Market Rate\n",
    "    \"CPI\": \"MEDCPIM158SFRBCLE\",  # Median Consumer Price Index\n",
    "    \"VIX\": \"VIXCLS\",  # CBOE Volatility Index\n",
    "    \"INDP\": \"INDPRO\",  # Industrial Production: Total Index\n",
    "    \"USHY_ADJ\": \"BAMLH0A0HYM2\",  # ICE BofA US High Yield Index Option-Adjusted Spread\n",
    "    \"US_LEADING\": \"USSLIND\",  # Leading Index for the United States\n",
    "    \"30Y_FRMTG\": \"MORTGAGE30US\",  # 30-Year Fixed Rate Mortgage Average in the United States\n",
    "    \"15Y_FRMTG\": \"MORTGAGE15US\",  # 15-Year Fixed Rate Mortgage Average in the United States\n",
    "    \"CPI_URBAN\": \"CUSR0000SEHA\",  # Consumer Price Index for All Urban Consumers: Rent of Primary Residence in U.S. City Average\n",
    "    \"RETAIL\": \"RSAFS\",  # Advance Retail Sales: Retail and Food Services, Total\n",
    "    \"PHARMA\": \"PCU32543254\",  # Producer Price Index by Industry: Pharmaceutical and Medicine Manufacturing\n",
    "    \"UNEMP\": \"UNRATE\",  # Unemployment Rate\n",
    "    \"UNEMP_PERM\": \"LNS13026638\",  # Unemployment Level - Permanent Job Losers\n",
    "    \"UNEMP_MEN\": \"LNS14000001\",  # Unemployment Rate - Men\n",
    "    \"UNEMP_WMN\": \"LNS14000002\",  # Unemployment Rate - Women\n",
    "    \"UNEMP_WHT\": \"LNS14000003\",  # Unemployment Rate - White\n",
    "    \"UNEMP_BLK\": \"LNS14000006\",  # Unemployment Rate - Black or African American\n",
    "    \"UNEMP_HIS\": \"LNS14000009\",  # Unemployment Rate - Hispanic or Latino\n",
    "    \"INC\": \"PI\",  # Personal Income\n",
    "    \"INC_DISP\": \"DSPIC96\",  # Real Disposable Personal Income\n",
    "    \"INC_DISP_PC\": \"A229RX0\",  # Real Disposable Personal Income: Per Capita\n",
    "    \"TAX_HIGH\": \"IITTRHB\",  # U.S Individual Income Tax: Tax Rates for Regular Tax: Highest Bracket\n",
    "    \"TAX_LOW\": \"IITTRLB\"  # U.S Individual Income Tax: Tax Rates for Regular Tax: Lowest Bracket\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba04396",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "for ticker in tickers:\n",
    "    for calculation in ['RET', 'VOL']:\n",
    "        features.append(f'{ticker}_1D_{calculation}')\n",
    "    for timeframe in ['1W', '1M', '3M', '6M']:\n",
    "        for calculation in ['RET', 'STD', 'VOL', 'GBM']:\n",
    "            features.append(f'{ticker}_{timeframe}_{calculation}')\n",
    "    for calculation in ['RET', 'STD', 'VOL']:\n",
    "        features.append(f'{ticker}_1Y_{calculation}')\n",
    "for indicator in indicators.keys():\n",
    "    features.append(indicator)\n",
    "\n",
    "targets = [f'{ticker}_TARGET' for ticker in tickers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c21011",
   "metadata": {},
   "source": [
    "#### Split the data into training, testing, and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3aac25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a dictionary with all market data in divided into sets and features/targets\n",
    "dates = mcal.get_calendar('NYSE').schedule(start_date='2004-01-01', end_date='2020-12-31').index\n",
    "market_data = dict({\n",
    "    \"X\" : pd.read_pickle(\"data/market_data.zip\").loc[:, features],\n",
    "    \"y\" : pd.read_pickle(\"data/market_data.zip\").loc[:, targets]\n",
    "})\n",
    "\n",
    "market_data[\"X_train\"] = market_data[\"X\"].loc['2004-01-01':'2015-12-31', :]\n",
    "market_data[\"y_train\"] = market_data[\"y\"].loc['2004-01-01':'2015-12-31', :]\n",
    "market_data[\"X_test\"] = market_data[\"X\"].loc['2016-01-01':'2020-12-31', :]\n",
    "market_data[\"y_test\"] = market_data[\"y\"].loc['2016-01-01':'2020-12-31', :]\n",
    "market_data[\"X\"] = market_data[\"X\"].loc['2004-01-01':'2020-12-31', :]\n",
    "market_data[\"y\"] = market_data[\"y\"].loc['2004-01-01':'2020-12-31', :]\n",
    "\n",
    "# Create split on train_all with -1 for training data and 0 for validation data (data after '2013-01-01')\n",
    "split = PredefinedSplit(test_fold=[0 if v else -1 for v in market_data[\"X_train\"].index < '2013-01-01'])\n",
    "\n",
    "def fill_invalid(df, fill):\n",
    "    df[df.isin([np.nan, np.inf, -1 * np.inf])] = fill\n",
    "\n",
    "# Additing Quantile columns then normalizing and scaling\n",
    "transformer = QuantileTransformer()\n",
    "market_data[\"X_train\"].loc[:, [column+\"_QUANTILE\" for column in market_data[\"X_train\"].columns]] = \\\n",
    "    pd.DataFrame(transformer.fit_transform(market_data[\"X_train\"]))\n",
    "market_data[\"X_test\"].loc[:, [column+\"_QUANTILE\" for column in market_data[\"X_test\"].columns]] = \\\n",
    "    pd.DataFrame(transformer.transform(market_data[\"X_test\"]))\n",
    "market_data[\"X\"].loc[:, [column+\"_QUANTILE\" for column in market_data[\"X\"].columns]] = \\\n",
    "    pd.DataFrame(transformer.transform(market_data[\"X\"]))\n",
    "\n",
    "fill_invalid(market_data[\"X_train\"], 0)\n",
    "fill_invalid(market_data[\"X_test\"], 0)\n",
    "fill_invalid(market_data[\"X\"], 0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "market_data[\"X_train\"] = pd.DataFrame(scaler.fit_transform(market_data[\"X_train\"]),\n",
    "                                      columns=market_data[\"X_train\"].columns, index=market_data[\"X_train\"].index)\n",
    "market_data[\"X_test\"] = pd.DataFrame(scaler.transform(market_data[\"X_test\"]),\n",
    "                                      columns=market_data[\"X_test\"].columns, index=market_data[\"X_test\"].index)\n",
    "market_data[\"X\"] = pd.DataFrame(scaler.transform(market_data[\"X\"]),\n",
    "                                      columns=market_data[\"X\"].columns, index=market_data[\"X\"].index)\n",
    "\n",
    "fill_invalid(market_data[\"X_train\"], 0)\n",
    "fill_invalid(market_data[\"X_test\"], 0)\n",
    "fill_invalid(market_data[\"X\"], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32031c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D input with each row being #time_steps x #feature_columns\n",
    "def create_dataset_3D (X, y, time_steps = 21):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(np.array(X[i:i + time_steps, :]))\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d64fa",
   "metadata": {},
   "source": [
    "### Define CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935998e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_CNN_simple():\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(21, 424)))\n",
    "    cnn.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "    cnn.add(Dropout(0.3))\n",
    "    cnn.add(MaxPool1D(pool_size=2))\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(128, activation='relu'))\n",
    "    cnn.add(Dropout(0.3))\n",
    "    cnn.add(Dense(64, activation='relu'))\n",
    "    cnn.add(Dropout(0.3))\n",
    "    cnn.add(Dense(32, activation='relu'))\n",
    "    cnn.add(Dropout(0.3))\n",
    "    cnn.add(Dense(16, activation='relu'))\n",
    "    cnn.add(Dropout(0.3))\n",
    "    cnn.add(Dense(9))\n",
    "    # Compile the model\n",
    "    cnn.compile(optimizer='adam', loss='mean_squared_error',\n",
    "                metrics=[MeanSquaredError(), RootMeanSquaredError()])\n",
    "    return cnn\n",
    "\n",
    "\n",
    "def build_CNN_multihead():\n",
    "    input_1 = Input(shape=(21, 424))\n",
    "    flat_1 = Flatten()(\n",
    "        MaxPool1D(pool_size=2)(\n",
    "            Dropout(0.3)(\n",
    "                Conv1D(filters=32, kernel_size=3, activation='relu')(\n",
    "                    Conv1D(filters=32, kernel_size=3, activation='relu')(input_1)))))\n",
    "    input_2 = Input(shape=(21, 424))\n",
    "    flat_2 = Flatten()(\n",
    "        MaxPool1D(pool_size=2)(\n",
    "            Dropout(0.3)(\n",
    "                Conv1D(filters=32, kernel_size=7, activation='relu')(\n",
    "                    Conv1D(filters=32, kernel_size=7, activation='relu')(input_2)))))\n",
    "    input_3 = Input(shape=(21, 424))\n",
    "    flat_3 = Flatten()(\n",
    "        MaxPool1D(pool_size=2)(\n",
    "            Dropout(0.3)(\n",
    "                Conv1D(filters=32, kernel_size=11, activation='relu')(\n",
    "                    Conv1D(filters=32, kernel_size=11, activation='relu')(input_3)))))\n",
    "    input_4 = Input(shape=(21, 424))\n",
    "    flat_4 = Flatten()(\n",
    "        MaxPool1D(pool_size=2)(\n",
    "            Dropout(0.3)(\n",
    "                Conv1D(filters=16, kernel_size=3, activation='relu')(\n",
    "                    Conv1D(filters=16, kernel_size=3, activation='relu')(input_4)))))\n",
    "    input_5 = Input(shape=(21, 424))\n",
    "    flat_5 = Flatten()(\n",
    "        MaxPool1D(pool_size=2)(\n",
    "            Dropout(0.3)(\n",
    "                Conv1D(filters=64, kernel_size=3, activation='relu')(\n",
    "                    Conv1D(filters=64, kernel_size=3, activation='relu')(input_5)))))\n",
    "    merged_heads = concatenate([flat_1, flat_2, flat_3, flat_4, flat_5])\n",
    "    dense_1 = Dense(128, activation='relu')(merged_heads)\n",
    "    dropout_1 = Dropout(0.3) (dense_1)\n",
    "    dense_2 = Dense(64, activation='relu')(dropout_1)\n",
    "    dropout_2 = Dropout(0.3) (dense_2)\n",
    "    dense_3 = Dense(32, activation='relu')(dropout_2)\n",
    "    dropout_3 = Dropout(0.3) (dense_3)\n",
    "    dense_4 = Dense(16, activation='relu')(dropout_3)\n",
    "    dropout_4 = Dropout(0.3) (dense_4)\n",
    "    output = Dense(9, activation='relu')(dropout_4)\n",
    "    \n",
    "    cnn = Model(inputs=[input_1, input_2, input_3, input_4, input_5], outputs=[output])\n",
    "    # Compile the model\n",
    "    cnn.compile(optimizer='adam', loss='mean_squared_error',\n",
    "                metrics=[MeanSquaredError(), RootMeanSquaredError()])\n",
    "    return cnn\n",
    "\n",
    "\n",
    "plot_model(build_CNN_simple(), to_file='img/simpleCNN.jpeg', show_shapes=True, show_layer_names=True)\n",
    "plot_model(build_CNN_multihead(), to_file='img/multi-headCNN.jpeg', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34dfeb3",
   "metadata": {},
   "source": [
    "### Fit and Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = create_dataset_3D(np.array(market_data[\"X_train\"]), np.array(market_data[\"y_train\"]))\n",
    "X_test, y_test = create_dataset_3D(np.array(market_data[\"X_test\"]), np.array(market_data[\"y_test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e75da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = KerasRegressor(build_fn=build_CNN_simple, nb_epoch=1e8, batch_size=32, verbose=False)\n",
    "simple_cnn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1725627",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_s = simple_cnn.predict(X_train)\n",
    "y_test_pred_s = simple_cnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0854cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_cnn = KerasRegressor(build_fn=build_CNN_multihead, nb_epoch=1e8, batch_size=32, verbose=False)\n",
    "multihead_cnn.fit([X_train] * 5, y_train)  # 5 coppied inputs, 1 input for each head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37df19c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_mh = multihead_cnn.predict([X_train] * 5)\n",
    "y_test_pred_mh = multihead_cnn.predict([X_test] * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363da4e1",
   "metadata": {},
   "source": [
    "### Regression and Classification and Financial Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3da98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metrics(y_true: np.array, y_pred: np.array, column_names: List[str]) -> np.array:\n",
    "    mse_list = [mean_squared_error(y_true[:, column], y_pred[:, column])\n",
    "                for column in range(len(column_names))]\n",
    "    mae_list = [median_absolute_error(y_true[:, column], y_pred[:, column])\n",
    "                for column in range(len(column_names))]\n",
    "    print(f\"Regression Metrics (Mean Squared Error and Median Absolute Error\")\n",
    "    for i, column in enumerate(column_names):\n",
    "        print(f\"- {column} Metrics\\n\"\n",
    "              f\"  - MSE: {mse_list[i]:.06f}\\n\"\n",
    "              f\"  - MAE: {mae_list[i]:.06f}\\n\"\n",
    "              f\"--------------------------------------------------\")\n",
    "    print(f\"- Average\\n\"\n",
    "          f\"  - MSE: {float(np.mean(mse_list)):.06f}\\n\"\n",
    "          f\"  - MAE: {float(np.mean(mae_list)):.06f}\\n\"\n",
    "          f\"--------------------------------------------------\")\n",
    "    print(f\"==================================================\\n\")\n",
    "\n",
    "\n",
    "print(\"Training Set:\")\n",
    "regression_metrics(y_train, y_train_pred, tickers)\n",
    "print(\"Testing Set:\")\n",
    "regression_metrics(y_test, y_test_pred, tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444479f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(y_true: np.array, y_pred: np.array, column_names: List[str]) -> np.array:\n",
    "    # create the positive negative classes in the targets\n",
    "    _y_true = [[-1 if n < 0 else 1 for n in row] for row in y_true]\n",
    "    _y_pred = [[-1 if n < 0 else 1 for n in row] for row in y_pred]\n",
    "\n",
    "    n_col = len(column_names)\n",
    "    cnf_mat_all = np.array([[0, 0], [0, 0]])\n",
    "    mean_metrics = {\"accuracy\":0.0, \"precision\":0.0, \"recall\":0.0, \"f1\":0.0}\n",
    "    confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "    print(f\"Classification Metrics (Mean Squared Error and Median Absolute Error\")\n",
    "    for i, column in enumerate(column_names):\n",
    "        cnf_mat = confusion_matrix(_y_true[:, column], _y_pred[:, column] )\n",
    "        accuracy = accuracy_score(_y_true[:, column], _y_pred[:, column] )\n",
    "        precision = precision_score(_y_true[:, column], _y_pred[:, column] )\n",
    "        recall = recall_score(_y_true[:, column], _y_pred[:, column] )\n",
    "        f1 = f1_score(_y_true[:, column], _y_pred[:, column] )\n",
    "        print(f\"- {column} Metrics\\n\"\n",
    "              f\"  - Confusion Matrix:\\n\"\n",
    "              f\"    [{cnf_mat[0]},\\n\"\n",
    "              f\"     {cnf_mat[1]}]\\n\"\n",
    "              f\"  - Accuracy:  {accuracy:.06f}\\n\"\n",
    "              f\"  - Precision: {precision:.06f}\\n\"\n",
    "              f\"  - Recall:    {recall:.06f}\\n\"\n",
    "              f\"  - F1-Score:  {f1:.06f}\\n\"\n",
    "              f\"--------------------------------------------------\")\n",
    "        cnf_mat_all = cnf_mat_all + np.array(cnf_mat)\n",
    "        mean_metrics[\"accuracy\"] += (accuracy / n_col)\n",
    "        mean_metrics[\"precision\"] += (precision / n_col)\n",
    "        mean_metrics[\"recall\"] += (recall / n_col)\n",
    "        mean_metrics[\"f1\"] += (f1 / n_col)\n",
    "\n",
    "    print(f\"- Average\\n\"\n",
    "          f\"  - Confusion Matrix:\\n\"\n",
    "          f\"    [{cnf_mat_all[0]},\\n\"\n",
    "          f\"     {cnf_mat_all[1]}]\\n\"\n",
    "          f\"  - Accuracy:  {mean_metrics[\"accuracy\"]:.06f}\\n\"\n",
    "          f\"  - Precision: {mean_metrics[\"precision\"]:.06f}\\n\"\n",
    "          f\"  - Recall:    {mean_metrics[\"recall\"]:.06f}\\n\"\n",
    "          f\"  - F1-Score:  {mean_metrics[\"f1\"]:.06f}\\n\"\n",
    "          f\"--------------------------------------------------\")\n",
    "    print(f\"==================================================\\n\")\n",
    "\n",
    "\n",
    "print(\"Training Set:\")\n",
    "classification_metrics(y_train, y_train_pred, tickers)\n",
    "print(\"Testing Set:\")\n",
    "classification_metrics(y_test, y_test_pred, tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "657a175d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05, 0.05, -0.05, -0.05, 0.05, -0.05], [0.05, -0.05, 0.05, -0.05, -0.05, 0.05]]\n"
     ]
    }
   ],
   "source": [
    "def cumulative_return(returns_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the cumulative returns for all columns in the returns DataFrame\n",
    "    :param returns_df: the DataFrame of returns\n",
    "    :return: a DataFrame of cumulative returns for each column\n",
    "    \"\"\"\n",
    "    cumulative_return_df = returns_df.fillna(0) + 1\n",
    "    return cumulative_return_df.cumprod(axis=0)\n",
    "\n",
    "\n",
    "def annualized_return(returns_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate an return the annualized return for all daily return columns in the given DataFrame\n",
    "    :param returns_df: the DataFrame of returns\n",
    "    :return: a series of annualized returns for each column\n",
    "    \"\"\"\n",
    "    n_days = len(returns_df.index)\n",
    "    returns = returns_df.add(1).prod(axis=0)\n",
    "    # annualized with the number of days and the total days per trading year\n",
    "    returns = (returns ** (252/n_days)).add(-1)\n",
    "    return returns\n",
    "\n",
    "\n",
    "def financial_metrics(y_true: np.array, y_pred: np.array, column_names: List[str], time_index: pd.DatetimeIndex,\n",
    "                      allocation: float=0.05) -> np.array:\n",
    "    \"\"\"\n",
    "    Takes a list of true returns and predicted returns. Uses the predicted returns to generate 5% allocation signals\n",
    "    to long/short the given asset. \n",
    "    Plots the adjusted returns of the columns individually, then the portfolio as a whole \n",
    "    (row mean of returns 2D array).\n",
    "    \"\"\"\n",
    "    signals = np.array([[-allocation if n < 0 else allocation for n in row] for row in y_pred])\n",
    "    adj_returns = y_true * signals\n",
    "    \n",
    "    # create dataframes of returns from the parameters and column names / dates\n",
    "    # divide returns by 21 (because each day is a 1-month look-ahead return)\n",
    "    df_b = pd.DataFrame(y_true / 21, columns=column_names, index=time_index)\n",
    "    df_b[\"Portfolio\"] = df_b.mean(axis=1)\n",
    "    df_m = pd.DataFrame(adj_returns / 21, columns=column_names, index=time_index)\n",
    "    df_m[\"Portfolio\"] = df_m.mean(axis=1)\n",
    "    \n",
    "    # generate annualized returns\n",
    "    df_b_ar = annualized_return(df_b)\n",
    "    df_m_ar = annualized_return(df_m)\n",
    "\n",
    "    # generate cumulative returns\n",
    "    df_b_c = cumulative_return(df_b)\n",
    "    df_m_c = cumulative_return(df_m)\n",
    "    \n",
    "    for column in df_b.columns:\n",
    "        df_plot = df_b.loc[:,[column]]\n",
    "        df_plot[column + \" Model\"] = df_m.loc[column]\n",
    "        f, ax = plt.subplots(figsize=(11, 9))\n",
    "        lineplot = sns.lineplot(data=df_plot, x=\"Time\", y=\"Cumulative Return (Starting = 1)\")\n",
    "        lineplot.set_title(f\"{column} Cumulative Return\")\n",
    "        fig = lineplot.get_figure()\n",
    "        fig.savefig(f\"img/{column}-return-cnn.jpeg\")\n",
    "        print(f\"- {column} Metrics\\n\"\n",
    "              f\"  - Annualized Return: \\n\"\n",
    "              f\"    - Benchmark: {df_b_ar[column]}\\n\"\n",
    "              f\"    - Model:     {df_m_ar[column]}\\n\"\n",
    "              f\"  - Volatility: \\n\"\n",
    "              f\"    - Benchmark: {float(np.std(df_b[column])):.06f}\\n\"\n",
    "              f\"    - Model:     {float(np.std(df_m[column])):.06f}\\n\"\n",
    "              f\"--------------------------------------------------\")\n",
    "        \n",
    "    print(f\"==================================================\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e361ca5e",
   "metadata": {},
   "source": [
    "### Metrics over Number of Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fb604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "powers = [i for i in range(1, 10)]\n",
    "epochs = sorted([5* 10**i for i in powers] + [10**i for i in powers])\n",
    "epoch_metrics = []\n",
    "for epoch in epochs:\n",
    "    epoch_model = KerasRegressor(build_fn=build_CNN, nb_epoch=epoch, batch_size=32, verbose=False)\n",
    "    epoch_model.fit(X_train, y_train)\n",
    "    y_train_pred = epoch_model.predict(X_train)\n",
    "    y_test_pred = epoch_model.predict(X_test)\n",
    "    mse_train_avg = np.mean([mean_squared_error(y_train[:, column], y_train_pred[:, column])\n",
    "                for column in range(len(tickers))])\n",
    "    mae_train_avg = np.mean([median_absolute_error(y_train[:, column], y_train_pred[:, column])\n",
    "                for column in range(len(tickers))])\n",
    "    \n",
    "    y_test_pred = epoch_model.predict(X_test)\n",
    "    y_test_pred = epoch_model.predict(X_test)\n",
    "    mse_test_avg = np.mean([mean_squared_error(y_test[:, column], y_test_pred[:, column])\n",
    "                for column in range(len(tickers))])\n",
    "    mae_test_avg = np.mean([median_absolute_error(y_test[:, column], y_test_pred[:, column])\n",
    "                for column in range(len(tickers))])\n",
    "    epoch_metrics.append({\"Epochs\": epoch, \"MSE\": mse_train_avg, \"MAE\": mae_train_avg, \"Set\": \"Train\"})\n",
    "    epoch_metrics.append({\"Epochs\": epoch, \"MSE\": mse_test_avg, \"MAE\": mae_test_avg, \"Set\": \"Test\"})\n",
    "epoch_metrics = pd.DataFrame(epoch_metrics)\n",
    "epoch_metrics.set_index(\"Epochs\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feedcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.semilogx(epochs, epoch_metrics.where(epoch_metrics.Set == \"Train\").dropna().MSE, label='Train')\n",
    "plt.semilogx(epochs, epoch_metrics.where(epoch_metrics.Set == \"Test\").dropna().MSE, label='Test')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim([0.0025, 0.01])\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Average MSE')\n",
    "\n",
    "# Show estimated coef_ vs true coef\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.semilogx(epochs, epoch_metrics.where(epoch_metrics.Set == \"Train\").dropna().MAE, label='Train')\n",
    "plt.semilogx(epochs, epoch_metrics.where(epoch_metrics.Set == \"Test\").dropna().MAE, label='Test')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim([0.025, 0.05])\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Average MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d6d6c",
   "metadata": {},
   "source": [
    "### Metrics over Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9592949",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [8, 16, 32, 64, 128, 256, 512]\n",
    "batch_metrics = []\n",
    "for batch in batches:\n",
    "    batch_model = KerasRegressor(build_fn=build_CNN, nb_epoch=1e8, batch_size=batch, verbose=False)\n",
    "    batch_model.fit(X_train, y_train)\n",
    "    y_train_pred = batch_model.predict(X_train)\n",
    "    y_test_pred = batch_model.predict(X_test)\n",
    "    mse_train_avg = np.mean([mean_squared_error(y_train[:, column], y_train_pred[:, column])\n",
    "                for column in range(len(tickers))])\n",
    "    mae_train_avg = np.mean([median_absolute_error(y_train[:, column], y_train_pred[:, column])\n",
    "                for column in range(len(tickers))])\n",
    "    \n",
    "    y_test_pred = batch_model.predict(X_test)\n",
    "    y_test_pred = batch_model.predict(X_test)\n",
    "    mse_test_avg = np.mean([mean_squared_error(y_test[:, column], y_test_pred[:, column])\n",
    "                for column in range(len(tickers))])\n",
    "    mae_test_avg = np.mean([median_absolute_error(y_test[:, column], y_test_pred[:, column])\n",
    "                for column in range(len(tickers))])\n",
    "    batch_metrics.append({\"Batch Size\": batch, \"MSE\": mse_train_avg, \"MAE\": mae_train_avg, \"Set\": \"Train\"})\n",
    "    batch_metrics.append({\"Batch Size\": batch, \"MSE\": mse_test_avg, \"MAE\": mae_test_avg, \"Set\": \"Test\"})\n",
    "batch_metrics = pd.DataFrame(batch_metrics)\n",
    "batch_metrics.set_index(\"Batch Size\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e82c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.semilogx(batches, batch_metrics.where(batch_metrics.Set == \"Train\").dropna().MSE, label='Train')\n",
    "plt.semilogx(batches, batch_metrics.where(batch_metrics.Set == \"Test\").dropna().MSE, label='Test')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim([0.0025, 0.01])\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Average MSE')\n",
    "\n",
    "# Show estimated coef_ vs true coef\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.semilogx(batches, batch_metrics.where(batch_metrics.Set == \"Train\").dropna().MAE, label='Train')\n",
    "plt.semilogx(batches, batch_metrics.where(batch_metrics.Set == \"Test\").dropna().MAE, label='Test')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim([0.025, 0.05])\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Average MAE')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
